{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the data --> 1115394\n",
      "The vocab size is 65\n",
      "Printing all the elements from vocab --> \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "with open('../data/shakespear.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Total length of the data -->\", len(text))\n",
    "\n",
    "chrs = sorted(list(set(text)))\n",
    "vocab_size = len(chrs)\n",
    "\n",
    "print(f\"The vocab size is {vocab_size}\")\n",
    "print(f\"Printing all the elements from vocab --> {''.join(chrs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Mappings\n",
    "stoi = {s:i for i, s in enumerate(chrs)}\n",
    "itos = {i:s for i, s in enumerate(chrs)}\n",
    "# print(stoi)\n",
    "# print(itos\n",
    "\n",
    "# def encode(context):\n",
    "#     out = []\n",
    "#     for s in context:\n",
    "#         out.append(stoi[s])\n",
    "#     return out\n",
    "\n",
    "# Takes string and returns list of integers\n",
    "encode = lambda context : [stoi[s] for s in context]\n",
    "# Takes list of integers and returns string\n",
    "decode = lambda ints : ''.join([itos[i] for i in ints])\n",
    "\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data --> 1115394 items\n",
      "Training data --> 1003854 items\n",
      "Validation data --> 111540 items\n"
     ]
    }
   ],
   "source": [
    "# Creating dataset\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "print(f\"Total data --> {data.shape[0]} items\")\n",
    "#print(data[0:400])\n",
    "\n",
    "# Train and val split\n",
    "n =  int(0.9 * len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]\n",
    "\n",
    "print(f\"Training data --> {train.shape[0]} items\")\n",
    "print(f\"Validation data --> {val.shape[0]} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "When input is [50] output is 42\n",
      "When input is [50, 42] output is 1\n",
      "When input is [50, 42, 1] output is 40\n",
      "When input is [50, 42, 1, 40] output is 43\n",
      "When input is [50, 42, 1, 40, 43] output is 1\n",
      "When input is [50, 42, 1, 40, 43, 1] output is 53\n",
      "When input is [50, 42, 1, 40, 43, 1, 53] output is 59\n",
      "When input is [50, 42, 1, 40, 43, 1, 53, 59] output is 56\n",
      "When input is [1] output is 57\n",
      "When input is [1, 57] output is 47\n",
      "When input is [1, 57, 47] output is 56\n",
      "When input is [1, 57, 47, 56] output is 6\n",
      "When input is [1, 57, 47, 56, 6] output is 1\n",
      "When input is [1, 57, 47, 56, 6, 1] output is 57\n",
      "When input is [1, 57, 47, 56, 6, 1, 57] output is 46\n",
      "When input is [1, 57, 47, 56, 6, 1, 57, 46] output is 53\n",
      "When input is [51] output is 40\n",
      "When input is [51, 40] output is 43\n",
      "When input is [51, 40, 43] output is 56\n",
      "When input is [51, 40, 43, 56] output is 1\n",
      "When input is [51, 40, 43, 56, 1] output is 61\n",
      "When input is [51, 40, 43, 56, 1, 61] output is 43\n",
      "When input is [51, 40, 43, 56, 1, 61, 43] output is 50\n",
      "When input is [51, 40, 43, 56, 1, 61, 43, 50] output is 50\n",
      "When input is [46] output is 43\n",
      "When input is [46, 43] output is 47\n",
      "When input is [46, 43, 47] output is 56\n",
      "When input is [46, 43, 47, 56] output is 1\n",
      "When input is [46, 43, 47, 56, 1] output is 41\n",
      "When input is [46, 43, 47, 56, 1, 41] output is 39\n",
      "When input is [46, 43, 47, 56, 1, 41, 39] output is 56\n",
      "When input is [46, 43, 47, 56, 1, 41, 39, 56] output is 43\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(800)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    xs = torch.stack([data[i : i+block_size] for i in ix])\n",
    "    ys = torch.stack([data[i+1 : i+block_size+1] for i in ix])\n",
    "    return xs, ys\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, 0:t+1]\n",
    "        target = yb[b, t]\n",
    "\n",
    "        print(f\"When input is {context.tolist()} output is {target.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(500)\n",
    "\n",
    "class BiGram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer which has it's __call__ function\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    # The nn.Module handles the __call__ func\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets shape (B, T)\n",
    "        logits = self.embedding_table(idx)\n",
    "\n",
    "        # Just in case we only want Logits while generating\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Logits that come out have shape [B, T, C]. For every batch, There are 8 characters and within these 8 characters, every\n",
    "            # charater is passed through the channels (lookup table) \n",
    "            B, T, C = logits.shape\n",
    "            # The cross_entropy loss takes in logits with shape [B, C]\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # Takes in idx -> Past context of shape [B, T] and predicts and appends the next token in context\n",
    "        \n",
    "        # \n",
    "        for _ in range(max_new_tokens):\n",
    "            # This will call the forward func\n",
    "            logits, loss = self(idx)\n",
    "            # Since it's bigram model, we care about the last timestep only, so we extract that\n",
    "            logits = logits[:, -1, :]       # noe dim is [B, C]\n",
    "            probs = F.softmax(logits, dim = -1)     # dim = -1 means last dim\n",
    "            ix = torch.multinomial(probs, num_samples = 1)\n",
    "            # Concatenating the next timestep \n",
    "            idx = torch.cat((idx, ix), dim = 1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.6475, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "yCjzq -?kl$XMBh.Fq:cEoP\n",
      "P:U bybATiG-Y\n",
      "NycCA3OpXnYXs!GyeSu;rSSej:u ;?UM!\n",
      ".Plzs!MMjvMjhETsZh,-BWFDAt?S\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "m = BiGram(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Torch oprimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / 50000      Loss -->    4.696692943572998\n",
      "Epoch   1000 / 50000      Loss -->    3.620892286300659\n",
      "Epoch   2000 / 50000      Loss -->    3.1328930854797363\n",
      "Epoch   3000 / 50000      Loss -->    2.791038990020752\n",
      "Epoch   4000 / 50000      Loss -->    2.7017123699188232\n",
      "Epoch   5000 / 50000      Loss -->    2.5440938472747803\n",
      "Epoch   6000 / 50000      Loss -->    2.4252851009368896\n",
      "Epoch   7000 / 50000      Loss -->    2.584836483001709\n",
      "Epoch   8000 / 50000      Loss -->    2.387962579727173\n",
      "Epoch   9000 / 50000      Loss -->    2.575676202774048\n",
      "Epoch   10000 / 50000      Loss -->    2.5128278732299805\n",
      "Epoch   11000 / 50000      Loss -->    2.4656758308410645\n",
      "Epoch   12000 / 50000      Loss -->    2.394345760345459\n",
      "Epoch   13000 / 50000      Loss -->    2.3631834983825684\n",
      "Epoch   14000 / 50000      Loss -->    2.4389445781707764\n",
      "Epoch   15000 / 50000      Loss -->    2.4022321701049805\n",
      "Epoch   16000 / 50000      Loss -->    2.4722418785095215\n",
      "Epoch   17000 / 50000      Loss -->    2.486950397491455\n",
      "Epoch   18000 / 50000      Loss -->    2.4348785877227783\n",
      "Epoch   19000 / 50000      Loss -->    2.560554265975952\n",
      "Epoch   20000 / 50000      Loss -->    2.3418173789978027\n",
      "Epoch   21000 / 50000      Loss -->    2.4393301010131836\n",
      "Epoch   22000 / 50000      Loss -->    2.346186876296997\n",
      "Epoch   23000 / 50000      Loss -->    2.4963533878326416\n",
      "Epoch   24000 / 50000      Loss -->    2.3911380767822266\n",
      "Epoch   25000 / 50000      Loss -->    2.4441721439361572\n",
      "Epoch   26000 / 50000      Loss -->    2.388727903366089\n",
      "Epoch   27000 / 50000      Loss -->    2.4032795429229736\n",
      "Epoch   28000 / 50000      Loss -->    2.415104627609253\n",
      "Epoch   29000 / 50000      Loss -->    2.3905186653137207\n",
      "Epoch   30000 / 50000      Loss -->    2.338639736175537\n",
      "Epoch   31000 / 50000      Loss -->    2.3820223808288574\n",
      "Epoch   32000 / 50000      Loss -->    2.5160460472106934\n",
      "Epoch   33000 / 50000      Loss -->    2.589956760406494\n",
      "Epoch   34000 / 50000      Loss -->    2.439422845840454\n",
      "Epoch   35000 / 50000      Loss -->    2.486461639404297\n",
      "Epoch   36000 / 50000      Loss -->    2.4946813583374023\n",
      "Epoch   37000 / 50000      Loss -->    2.3753950595855713\n",
      "Epoch   38000 / 50000      Loss -->    2.524275779724121\n",
      "Epoch   39000 / 50000      Loss -->    2.5052475929260254\n",
      "Epoch   40000 / 50000      Loss -->    2.6391613483428955\n",
      "Epoch   41000 / 50000      Loss -->    2.3795270919799805\n",
      "Epoch   42000 / 50000      Loss -->    2.4504008293151855\n",
      "Epoch   43000 / 50000      Loss -->    2.442854166030884\n",
      "Epoch   44000 / 50000      Loss -->    2.4225282669067383\n",
      "Epoch   45000 / 50000      Loss -->    2.4694066047668457\n",
      "Epoch   46000 / 50000      Loss -->    2.399770736694336\n",
      "Epoch   47000 / 50000      Loss -->    2.3789761066436768\n",
      "Epoch   48000 / 50000      Loss -->    2.509171962738037\n",
      "Epoch   49000 / 50000      Loss -->    2.4350712299346924\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Get a batch from training data\n",
    "    xb, yb = get_batch('train')\n",
    "    # Forward Pass\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # Set zero grad and perform backprop\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch   {epoch} / {epochs}      Loss -->    {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "G myonerey fowherde y wil thorut t ten withtades, ire hewe Wid fat istis s aveathas ggr:\n",
      "Paistwesh t\n"
     ]
    }
   ],
   "source": [
    "# Generate a text starting with \\n as the first character\n",
    "torch.manual_seed(80)\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toy example of aggregating tokens\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42)\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "head_size = 16\n",
    "\n",
    "# Random trial batch of inputs of shape [B,T,C]\n",
    "x = torch.randn(B, T, C)     # B,T,C --> 4,8,3\n",
    "\n",
    "# Attention head layers\n",
    "key = nn.Linear(C, head_size, bias = False)\n",
    "query = nn.Linear(C, head_size, bias = False)\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "# Get key and Query vectors by feedforwarding the input\n",
    "k = key(x)      # [B, T, head_size]\n",
    "q = query(x)    # [B, T, head_size]\n",
    "\n",
    "wei = q @ k.transpose(-1, -2) * (head_size**-0.5)      # [B, T, head_size] @ [B, head_size, T] -> [B, T, T] (Weight matrix for every item in the batch)\n",
    "\n",
    "# A Lower triangle 1.0\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# Initializing weights to zero\n",
    "# wei = torch.zeros_like(tril).float()\n",
    "# Preventing Future tokens talking to current and past tokens\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# Normalizing to get averages across time dimension\n",
    "wei = wei.softmax(dim = -1)\n",
    "\n",
    "# Getting the value vector\n",
    "v = value(x)      # [B, T, head_size]\n",
    "\n",
    "# Talking of weights with value\n",
    "out = wei @ v     # [B,T,T] @ [B,T,head_size] -> [B,T,head_size]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7630, -0.2412, -0.4150,  0.3833,  0.5740, -1.6738,  0.7954,  0.6872,\n",
       "         -0.3848,  0.5073, -0.5312, -0.1221,  0.0445,  1.2169,  0.9940,  1.5281],\n",
       "        [ 0.4058, -0.0920, -0.7653, -0.5147,  0.1817, -0.4080,  0.0756, -0.7033,\n",
       "         -0.0571,  0.3145,  0.3326,  0.0922,  0.1446,  0.5214,  0.3781, -0.1178],\n",
       "        [ 0.2012,  0.0409, -0.1103,  0.3876,  0.6604, -0.8814,  0.2189,  0.0529,\n",
       "         -0.4067,  0.3265, -0.1413, -0.2490, -0.4813,  0.5791,  0.9548,  1.0026],\n",
       "        [ 0.0370,  0.2438, -0.1707, -0.0168, -0.0221, -0.3756, -0.1570, -0.6721,\n",
       "         -0.1865,  0.2293,  0.1447,  0.1949,  0.2877,  0.4271,  0.1980,  0.0253],\n",
       "        [ 0.2009,  0.1195, -0.2142,  0.3468,  0.1683, -0.8404,  0.0235, -0.0529,\n",
       "         -0.1590,  0.2322, -0.2571,  0.0770,  0.1777,  0.6503,  0.4119,  0.5479],\n",
       "        [-0.0446,  0.1640, -0.3607,  0.1286,  0.0677, -0.2968, -0.4088, -0.4496,\n",
       "          0.0718,  0.0469,  0.0142,  0.1075,  0.0830,  0.2601,  0.1443, -0.1559],\n",
       "        [ 0.1647,  0.0560, -0.1168,  0.4240,  0.5889, -0.8504,  0.1198,  0.0650,\n",
       "         -0.3278,  0.2436, -0.2167, -0.2715, -0.4648,  0.5273,  0.8451,  0.9254],\n",
       "        [ 0.0073, -0.0154,  0.0039, -0.4113, -0.2581, -0.4842,  0.0550, -0.3113,\n",
       "         -0.2439, -0.0607,  0.1581, -0.1452,  0.2576,  0.2912, -0.1158,  0.1573]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
