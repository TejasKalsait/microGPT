{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the data --> 1115394\n",
      "The vocab size is 65\n",
      "Printing all the elements from vocab --> \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "with open('../data/shakespear.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Total length of the data -->\", len(text))\n",
    "\n",
    "chrs = sorted(list(set(text)))\n",
    "vocab_size = len(chrs)\n",
    "\n",
    "print(f\"The vocab size is {vocab_size}\")\n",
    "print(f\"Printing all the elements from vocab --> {''.join(chrs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Mappings\n",
    "stoi = {s:i for i, s in enumerate(chrs)}\n",
    "itos = {i:s for i, s in enumerate(chrs)}\n",
    "# print(stoi)\n",
    "# print(itos\n",
    "\n",
    "# def encode(context):\n",
    "#     out = []\n",
    "#     for s in context:\n",
    "#         out.append(stoi[s])\n",
    "#     return out\n",
    "\n",
    "# Takes string and returns list of integers\n",
    "encode = lambda context : [stoi[s] for s in context]\n",
    "# Takes list of integers and returns string\n",
    "decode = lambda ints : ''.join([itos[i] for i in ints])\n",
    "\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data --> 1115394 items\n",
      "Training data --> 1003854 items\n",
      "Validation data --> 111540 items\n"
     ]
    }
   ],
   "source": [
    "# Creating dataset\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "print(f\"Total data --> {data.shape[0]} items\")\n",
    "#print(data[0:400])\n",
    "\n",
    "# Train and val split\n",
    "n =  int(0.9 * len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]\n",
    "\n",
    "print(f\"Training data --> {train.shape[0]} items\")\n",
    "print(f\"Validation data --> {val.shape[0]} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "When input is [50] output is 42\n",
      "When input is [50, 42] output is 1\n",
      "When input is [50, 42, 1] output is 40\n",
      "When input is [50, 42, 1, 40] output is 43\n",
      "When input is [50, 42, 1, 40, 43] output is 1\n",
      "When input is [50, 42, 1, 40, 43, 1] output is 53\n",
      "When input is [50, 42, 1, 40, 43, 1, 53] output is 59\n",
      "When input is [50, 42, 1, 40, 43, 1, 53, 59] output is 56\n",
      "When input is [1] output is 57\n",
      "When input is [1, 57] output is 47\n",
      "When input is [1, 57, 47] output is 56\n",
      "When input is [1, 57, 47, 56] output is 6\n",
      "When input is [1, 57, 47, 56, 6] output is 1\n",
      "When input is [1, 57, 47, 56, 6, 1] output is 57\n",
      "When input is [1, 57, 47, 56, 6, 1, 57] output is 46\n",
      "When input is [1, 57, 47, 56, 6, 1, 57, 46] output is 53\n",
      "When input is [51] output is 40\n",
      "When input is [51, 40] output is 43\n",
      "When input is [51, 40, 43] output is 56\n",
      "When input is [51, 40, 43, 56] output is 1\n",
      "When input is [51, 40, 43, 56, 1] output is 61\n",
      "When input is [51, 40, 43, 56, 1, 61] output is 43\n",
      "When input is [51, 40, 43, 56, 1, 61, 43] output is 50\n",
      "When input is [51, 40, 43, 56, 1, 61, 43, 50] output is 50\n",
      "When input is [46] output is 43\n",
      "When input is [46, 43] output is 47\n",
      "When input is [46, 43, 47] output is 56\n",
      "When input is [46, 43, 47, 56] output is 1\n",
      "When input is [46, 43, 47, 56, 1] output is 41\n",
      "When input is [46, 43, 47, 56, 1, 41] output is 39\n",
      "When input is [46, 43, 47, 56, 1, 41, 39] output is 56\n",
      "When input is [46, 43, 47, 56, 1, 41, 39, 56] output is 43\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(800)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    xs = torch.stack([data[i : i+block_size] for i in ix])\n",
    "    ys = torch.stack([data[i+1 : i+block_size+1] for i in ix])\n",
    "    return xs, ys\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, 0:t+1]\n",
    "        target = yb[b, t]\n",
    "\n",
    "        print(f\"When input is {context.tolist()} output is {target.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(500)\n",
    "\n",
    "class BiGram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer which has it's __call__ function\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    # The nn.Module handles the __call__ func\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets shape (B, T)\n",
    "        logits = self.embedding_table(idx)\n",
    "\n",
    "        # Just in case we only want Logits while generating\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Logits that come out have shape [B, T, C]. For every batch, There are 8 characters and within these 8 characters, every\n",
    "            # charater is passed through the channels (lookup table) \n",
    "            B, T, C = logits.shape\n",
    "            # The cross_entropy loss takes in logits with shape [B, C]\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # Takes in idx -> Past context of shape [B, T] and predicts and appends the next token in context\n",
    "        \n",
    "        # \n",
    "        for _ in range(max_new_tokens):\n",
    "            # This will call the forward func\n",
    "            logits, loss = self(idx)\n",
    "            # Since it's bigram model, we care about the last timestep only, so we extract that\n",
    "            logits = logits[:, -1, :]       # noe dim is [B, C]\n",
    "            probs = F.softmax(logits, dim = -1)     # dim = -1 means last dim\n",
    "            ix = torch.multinomial(probs, num_samples = 1)\n",
    "            # Concatenating the next timestep \n",
    "            idx = torch.cat((idx, ix), dim = 1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.6475, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "yCjzq -?kl$XMBh.Fq:cEoP\n",
      "P:U bybATiG-Y\n",
      "NycCA3OpXnYXs!GyeSu;rSSej:u ;?UM!\n",
      ".Plzs!MMjvMjhETsZh,-BWFDAt?S\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "m = BiGram(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Torch oprimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / 50000      Loss -->    4.696692943572998\n",
      "Epoch   1000 / 50000      Loss -->    3.620892286300659\n",
      "Epoch   2000 / 50000      Loss -->    3.1328930854797363\n",
      "Epoch   3000 / 50000      Loss -->    2.791038990020752\n",
      "Epoch   4000 / 50000      Loss -->    2.7017123699188232\n",
      "Epoch   5000 / 50000      Loss -->    2.5440938472747803\n",
      "Epoch   6000 / 50000      Loss -->    2.4252851009368896\n",
      "Epoch   7000 / 50000      Loss -->    2.584836483001709\n",
      "Epoch   8000 / 50000      Loss -->    2.387962579727173\n",
      "Epoch   9000 / 50000      Loss -->    2.575676202774048\n",
      "Epoch   10000 / 50000      Loss -->    2.5128278732299805\n",
      "Epoch   11000 / 50000      Loss -->    2.4656758308410645\n",
      "Epoch   12000 / 50000      Loss -->    2.394345760345459\n",
      "Epoch   13000 / 50000      Loss -->    2.3631834983825684\n",
      "Epoch   14000 / 50000      Loss -->    2.4389445781707764\n",
      "Epoch   15000 / 50000      Loss -->    2.4022321701049805\n",
      "Epoch   16000 / 50000      Loss -->    2.4722418785095215\n",
      "Epoch   17000 / 50000      Loss -->    2.486950397491455\n",
      "Epoch   18000 / 50000      Loss -->    2.4348785877227783\n",
      "Epoch   19000 / 50000      Loss -->    2.560554265975952\n",
      "Epoch   20000 / 50000      Loss -->    2.3418173789978027\n",
      "Epoch   21000 / 50000      Loss -->    2.4393301010131836\n",
      "Epoch   22000 / 50000      Loss -->    2.346186876296997\n",
      "Epoch   23000 / 50000      Loss -->    2.4963533878326416\n",
      "Epoch   24000 / 50000      Loss -->    2.3911380767822266\n",
      "Epoch   25000 / 50000      Loss -->    2.4441721439361572\n",
      "Epoch   26000 / 50000      Loss -->    2.388727903366089\n",
      "Epoch   27000 / 50000      Loss -->    2.4032795429229736\n",
      "Epoch   28000 / 50000      Loss -->    2.415104627609253\n",
      "Epoch   29000 / 50000      Loss -->    2.3905186653137207\n",
      "Epoch   30000 / 50000      Loss -->    2.338639736175537\n",
      "Epoch   31000 / 50000      Loss -->    2.3820223808288574\n",
      "Epoch   32000 / 50000      Loss -->    2.5160460472106934\n",
      "Epoch   33000 / 50000      Loss -->    2.589956760406494\n",
      "Epoch   34000 / 50000      Loss -->    2.439422845840454\n",
      "Epoch   35000 / 50000      Loss -->    2.486461639404297\n",
      "Epoch   36000 / 50000      Loss -->    2.4946813583374023\n",
      "Epoch   37000 / 50000      Loss -->    2.3753950595855713\n",
      "Epoch   38000 / 50000      Loss -->    2.524275779724121\n",
      "Epoch   39000 / 50000      Loss -->    2.5052475929260254\n",
      "Epoch   40000 / 50000      Loss -->    2.6391613483428955\n",
      "Epoch   41000 / 50000      Loss -->    2.3795270919799805\n",
      "Epoch   42000 / 50000      Loss -->    2.4504008293151855\n",
      "Epoch   43000 / 50000      Loss -->    2.442854166030884\n",
      "Epoch   44000 / 50000      Loss -->    2.4225282669067383\n",
      "Epoch   45000 / 50000      Loss -->    2.4694066047668457\n",
      "Epoch   46000 / 50000      Loss -->    2.399770736694336\n",
      "Epoch   47000 / 50000      Loss -->    2.3789761066436768\n",
      "Epoch   48000 / 50000      Loss -->    2.509171962738037\n",
      "Epoch   49000 / 50000      Loss -->    2.4350712299346924\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Get a batch from training data\n",
    "    xb, yb = get_batch('train')\n",
    "    # Forward Pass\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # Set zero grad and perform backprop\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch   {epoch} / {epochs}      Loss -->    {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "G myonerey fowherde y wil thorut t ten withtades, ire hewe Wid fat istis s aveathas ggr:\n",
      "Paistwesh t\n"
     ]
    }
   ],
   "source": [
    "# Generate a text starting with \\n as the first character\n",
    "torch.manual_seed(80)\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[39.0000, 52.0000, 29.0000],\n",
       "         [27.0000, 47.5000, 18.5000],\n",
       "         [38.3333, 38.6667, 25.3333],\n",
       "         [43.2500, 33.7500, 24.7500],\n",
       "         [36.8000, 29.2000, 24.6000],\n",
       "         [39.5000, 30.3333, 27.1667],\n",
       "         [37.2857, 26.4286, 26.4286],\n",
       "         [39.2500, 23.3750, 26.1250]],\n",
       "\n",
       "        [[44.0000, 30.0000, 38.0000],\n",
       "         [23.0000, 47.0000, 49.0000],\n",
       "         [22.3333, 42.3333, 36.6667],\n",
       "         [31.2500, 37.2500, 42.7500],\n",
       "         [33.8000, 34.8000, 44.0000],\n",
       "         [32.6667, 38.8333, 47.1667],\n",
       "         [34.0000, 42.4286, 44.4286],\n",
       "         [37.2500, 39.1250, 40.7500]],\n",
       "\n",
       "        [[62.0000, 62.0000, 47.0000],\n",
       "         [62.0000, 56.5000, 45.5000],\n",
       "         [59.6667, 55.0000, 51.6667],\n",
       "         [59.0000, 42.0000, 48.0000],\n",
       "         [57.4000, 35.0000, 42.6000],\n",
       "         [49.3333, 35.6667, 38.5000],\n",
       "         [42.8571, 34.1429, 41.5714],\n",
       "         [39.2500, 36.1250, 43.6250]],\n",
       "\n",
       "        [[ 9.0000, 26.0000, 53.0000],\n",
       "         [ 5.5000, 23.0000, 40.5000],\n",
       "         [19.3333, 35.3333, 29.3333],\n",
       "         [25.5000, 41.7500, 24.0000],\n",
       "         [29.8000, 40.4000, 22.0000],\n",
       "         [27.6667, 39.6667, 26.6667],\n",
       "         [29.4286, 34.5714, 23.1429],\n",
       "         [33.5000, 31.0000, 27.0000]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toy example of aggregating tokens\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "B, T, C = 4, 8, 3\n",
    "\n",
    "# Random trial batch of inputs of shape [B,T,C]\n",
    "x = torch.randint(1, 65, (B, T, C)).float()     # B,T,C --> 4,8,3\n",
    "print(x.shape)\n",
    "\n",
    "# A Lower triangle 1.0\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# Initializing weights to zero\n",
    "wei = torch.zeros_like(tril).float()\n",
    "# Preventing Future tokens talking to current and past tokens\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# Normalizing to get averages across time dimension\n",
    "wei = wei.softmax(dim = -1)\n",
    "# Talking of weights with the input\n",
    "y = wei @ x     # [T,T] @ [B,T,C] -> [B,T,C]\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
