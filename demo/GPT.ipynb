{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the data --> 1115394\n",
      "The vocab size is 65\n",
      "Printing all the elements from vocab --> \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "with open('../data/shakespear.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Total length of the data -->\", len(text))\n",
    "\n",
    "chrs = sorted(list(set(text)))\n",
    "vocab_size = len(chrs)\n",
    "\n",
    "print(f\"The vocab size is {vocab_size}\")\n",
    "print(f\"Printing all the elements from vocab --> {''.join(chrs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Mappings\n",
    "stoi = {s:i for i, s in enumerate(chrs)}\n",
    "itos = {i:s for i, s in enumerate(chrs)}\n",
    "# print(stoi)\n",
    "# print(itos\n",
    "\n",
    "# def encode(context):\n",
    "#     out = []\n",
    "#     for s in context:\n",
    "#         out.append(stoi[s])\n",
    "#     return out\n",
    "\n",
    "# Takes string and returns list of integers\n",
    "encode = lambda context : [stoi[s] for s in context]\n",
    "# Takes list of integers and returns string\n",
    "decode = lambda ints : ''.join([itos[i] for i in ints])\n",
    "\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data --> 1115394 items\n",
      "Training data --> 1003854 items\n",
      "Validation data --> 111540 items\n"
     ]
    }
   ],
   "source": [
    "# Creating dataset\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "print(f\"Total data --> {data.shape[0]} items\")\n",
    "#print(data[0:400])\n",
    "\n",
    "# Train and val split\n",
    "n =  int(0.9 * len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]\n",
    "\n",
    "print(f\"Training data --> {train.shape[0]} items\")\n",
    "print(f\"Validation data --> {val.shape[0]} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "When input is [50] output is 42\n",
      "When input is [50, 42] output is 1\n",
      "When input is [50, 42, 1] output is 40\n",
      "When input is [50, 42, 1, 40] output is 43\n",
      "When input is [50, 42, 1, 40, 43] output is 1\n",
      "When input is [50, 42, 1, 40, 43, 1] output is 53\n",
      "When input is [50, 42, 1, 40, 43, 1, 53] output is 59\n",
      "When input is [50, 42, 1, 40, 43, 1, 53, 59] output is 56\n",
      "When input is [1] output is 57\n",
      "When input is [1, 57] output is 47\n",
      "When input is [1, 57, 47] output is 56\n",
      "When input is [1, 57, 47, 56] output is 6\n",
      "When input is [1, 57, 47, 56, 6] output is 1\n",
      "When input is [1, 57, 47, 56, 6, 1] output is 57\n",
      "When input is [1, 57, 47, 56, 6, 1, 57] output is 46\n",
      "When input is [1, 57, 47, 56, 6, 1, 57, 46] output is 53\n",
      "When input is [51] output is 40\n",
      "When input is [51, 40] output is 43\n",
      "When input is [51, 40, 43] output is 56\n",
      "When input is [51, 40, 43, 56] output is 1\n",
      "When input is [51, 40, 43, 56, 1] output is 61\n",
      "When input is [51, 40, 43, 56, 1, 61] output is 43\n",
      "When input is [51, 40, 43, 56, 1, 61, 43] output is 50\n",
      "When input is [51, 40, 43, 56, 1, 61, 43, 50] output is 50\n",
      "When input is [46] output is 43\n",
      "When input is [46, 43] output is 47\n",
      "When input is [46, 43, 47] output is 56\n",
      "When input is [46, 43, 47, 56] output is 1\n",
      "When input is [46, 43, 47, 56, 1] output is 41\n",
      "When input is [46, 43, 47, 56, 1, 41] output is 39\n",
      "When input is [46, 43, 47, 56, 1, 41, 39] output is 56\n",
      "When input is [46, 43, 47, 56, 1, 41, 39, 56] output is 43\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(800)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "\n",
    "    data = train if split == 'train' else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    xs = torch.stack([data[i : i+block_size] for i in ix])\n",
    "    ys = torch.stack([data[i+1 : i+block_size+1] for i in ix])\n",
    "    return xs, ys\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, 0:t+1]\n",
    "        target = yb[b, t]\n",
    "\n",
    "        print(f\"When input is {context.tolist()} output is {target.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(500)\n",
    "\n",
    "class BiGram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer which has it's __call__ function\n",
    "        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    # The nn.Module handles the __call__ func\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets shape (B, T)\n",
    "        logits = self.embedding_table(idx)\n",
    "\n",
    "        # Just in case we only want Logits while generating\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Logits that come out have shape [B, T, C]. For every batch, There are 8 characters and within these 8 characters, every\n",
    "            # charater is passed through the channels (lookup table) \n",
    "            B, T, C = logits.shape\n",
    "            # The cross_entropy loss takes in logits with shape [B, C]\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # Takes in idx -> Past context of shape [B, T] and predicts and appends the next token in context\n",
    "        \n",
    "        # \n",
    "        for _ in range(max_new_tokens):\n",
    "            # This will call the forward func\n",
    "            logits, loss = self(idx)\n",
    "            # Since it's bigram model, we care about the last timestep only, so we extract that\n",
    "            logits = logits[:, -1, :]       # noe dim is [B, C]\n",
    "            probs = F.softmax(logits, dim = -1)     # dim = -1 means last dim\n",
    "            ix = torch.multinomial(probs, num_samples = 1)\n",
    "            # Concatenating the next timestep \n",
    "            idx = torch.cat((idx, ix), dim = 1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.7353, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "ONb;Yh$E-WA3wV\n",
      "PgYmwDxBOETBiscuY.DYv-S3.owQMwF!DOFiQ,FjzIp'ydHn'ONKO3Gmd3IZOX hE?V;CSNrFa!RGAjxf'CIv\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "m = BiGram(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Torch oprimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / 50000      Loss -->    4.550746440887451\n",
      "Epoch   1000 / 50000      Loss -->    3.724834442138672\n",
      "Epoch   2000 / 50000      Loss -->    3.1005749702453613\n",
      "Epoch   3000 / 50000      Loss -->    2.831899642944336\n",
      "Epoch   4000 / 50000      Loss -->    2.640024423599243\n",
      "Epoch   5000 / 50000      Loss -->    2.524759292602539\n",
      "Epoch   6000 / 50000      Loss -->    2.531472682952881\n",
      "Epoch   7000 / 50000      Loss -->    2.402622938156128\n",
      "Epoch   8000 / 50000      Loss -->    2.5160813331604004\n",
      "Epoch   9000 / 50000      Loss -->    2.514781951904297\n",
      "Epoch   10000 / 50000      Loss -->    2.5321924686431885\n",
      "Epoch   11000 / 50000      Loss -->    2.358546018600464\n",
      "Epoch   12000 / 50000      Loss -->    2.416508674621582\n",
      "Epoch   13000 / 50000      Loss -->    2.4953176975250244\n",
      "Epoch   14000 / 50000      Loss -->    2.390571355819702\n",
      "Epoch   15000 / 50000      Loss -->    2.5223395824432373\n",
      "Epoch   16000 / 50000      Loss -->    2.4284257888793945\n",
      "Epoch   17000 / 50000      Loss -->    2.501702308654785\n",
      "Epoch   18000 / 50000      Loss -->    2.530519723892212\n",
      "Epoch   19000 / 50000      Loss -->    2.450986862182617\n",
      "Epoch   20000 / 50000      Loss -->    2.355429172515869\n",
      "Epoch   21000 / 50000      Loss -->    2.4400248527526855\n",
      "Epoch   22000 / 50000      Loss -->    2.426680564880371\n",
      "Epoch   23000 / 50000      Loss -->    2.4057765007019043\n",
      "Epoch   24000 / 50000      Loss -->    2.371328830718994\n",
      "Epoch   25000 / 50000      Loss -->    2.4844255447387695\n",
      "Epoch   26000 / 50000      Loss -->    2.43331241607666\n",
      "Epoch   27000 / 50000      Loss -->    2.227236747741699\n",
      "Epoch   28000 / 50000      Loss -->    2.3491365909576416\n",
      "Epoch   29000 / 50000      Loss -->    2.4181878566741943\n",
      "Epoch   30000 / 50000      Loss -->    2.4411230087280273\n",
      "Epoch   31000 / 50000      Loss -->    2.3896055221557617\n",
      "Epoch   32000 / 50000      Loss -->    2.5573618412017822\n",
      "Epoch   33000 / 50000      Loss -->    2.4672772884368896\n",
      "Epoch   34000 / 50000      Loss -->    2.534505844116211\n",
      "Epoch   35000 / 50000      Loss -->    2.4554860591888428\n",
      "Epoch   36000 / 50000      Loss -->    2.357297420501709\n",
      "Epoch   37000 / 50000      Loss -->    2.480095148086548\n",
      "Epoch   38000 / 50000      Loss -->    2.482936143875122\n",
      "Epoch   39000 / 50000      Loss -->    2.5152862071990967\n",
      "Epoch   40000 / 50000      Loss -->    2.385003089904785\n",
      "Epoch   41000 / 50000      Loss -->    2.552635431289673\n",
      "Epoch   42000 / 50000      Loss -->    2.4213602542877197\n",
      "Epoch   43000 / 50000      Loss -->    2.4379353523254395\n",
      "Epoch   44000 / 50000      Loss -->    2.4839487075805664\n",
      "Epoch   45000 / 50000      Loss -->    2.346860885620117\n",
      "Epoch   46000 / 50000      Loss -->    2.3813955783843994\n",
      "Epoch   47000 / 50000      Loss -->    2.433654308319092\n",
      "Epoch   48000 / 50000      Loss -->    2.5352821350097656\n",
      "Epoch   49000 / 50000      Loss -->    2.4585120677948\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Get a batch from training data\n",
    "    xb, yb = get_batch('train')\n",
    "    # Forward Pass\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # Set zero grad and perform backprop\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch   {epoch} / {epochs}      Loss -->    {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wave, de woupan br\n",
      "QUK:\n",
      "ARUKE:\n",
      "ESlithaby, VInl mous tave!\n",
      "f spe:\n",
      "\n",
      "\n",
      "Frkefove thepiers hanket t s,\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "# Generate a text starting with \\n as the first character\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
